{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "c8c398c31884415489cd065b696416f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5f5bdf656a784f97aa922ad91156be9f",
              "IPY_MODEL_a4fa5d88e91d4f5b9a12e4cb3eb0ae43",
              "IPY_MODEL_3661f1db92864aa08a2b52d96721c7e7"
            ],
            "layout": "IPY_MODEL_30894df989454bceac3f3f533a2233e5"
          }
        },
        "5f5bdf656a784f97aa922ad91156be9f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3771fd641bac4cf29c30b6f08f28eafa",
            "placeholder": "​",
            "style": "IPY_MODEL_8d69125fb3e5486e8145b4a4c63002b4",
            "value": "Map: 100%"
          }
        },
        "a4fa5d88e91d4f5b9a12e4cb3eb0ae43": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_19c39f174f40438cb1e2807ad21e35ba",
            "max": 819,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_14cfe78bcee445609d6be45b249a3ce2",
            "value": 819
          }
        },
        "3661f1db92864aa08a2b52d96721c7e7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a72e3e6412694f13afdecc7f18ad6fc2",
            "placeholder": "​",
            "style": "IPY_MODEL_08686dc6a601411496858fffd21cb65b",
            "value": " 819/819 [00:00&lt;00:00, 1804.70 examples/s]"
          }
        },
        "30894df989454bceac3f3f533a2233e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3771fd641bac4cf29c30b6f08f28eafa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d69125fb3e5486e8145b4a4c63002b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19c39f174f40438cb1e2807ad21e35ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14cfe78bcee445609d6be45b249a3ce2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a72e3e6412694f13afdecc7f18ad6fc2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "08686dc6a601411496858fffd21cb65b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6nNB_KNwGL34",
        "outputId": "5e9c7fc4-a953-4afa-d618-3c21a4328753"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tue Jul  9 16:24:25 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   71C    P8              11W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "|  No running processes found                                                           |\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline, set_seed\n",
        "from datasets import load_dataset, load_from_disk, load_metric\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "\n",
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izo4E0PaG16N",
        "outputId": "7e464b77-c036-4475-e8d4-fa9429a08631"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "model_ckpt = \"google/pegasus-cnn_dailymail\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_ckpt)\n",
        "\n",
        "model_pegasus = AutoModelForSeq2SeqLM.from_pretrained(model_ckpt).to(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4_FZrVVCIZsb",
        "outputId": "5e2eb9a8-218e-4755-d712-feef0dc63cd0"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of PegasusForConditionalGeneration were not initialized from the model checkpoint at google/pegasus-cnn_dailymail and are newly initialized: ['model.decoder.embed_positions.weight', 'model.encoder.embed_positions.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the dataset\n",
        "!wget https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
        "!unzip summarizer-data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4PmcSPj0I6aZ",
        "outputId": "7e21004b-364b-4d5c-b887-d878079eed7d"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-07-09 16:20:03--  https://github.com/entbappy/Branching-tutorial/raw/master/summarizer-data.zip\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip [following]\n",
            "--2024-07-09 16:20:04--  https://raw.githubusercontent.com/entbappy/Branching-tutorial/master/summarizer-data.zip\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.108.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 7903594 (7.5M) [application/zip]\n",
            "Saving to: ‘summarizer-data.zip.1’\n",
            "\n",
            "summarizer-data.zip 100%[===================>]   7.54M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-07-09 16:20:04 (309 MB/s) - ‘summarizer-data.zip.1’ saved [7903594/7903594]\n",
            "\n",
            "Archive:  summarizer-data.zip\n",
            "replace samsum-test.csv? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -all"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckE3kGYWKfRd",
        "outputId": "9629e0c1-a338-4155-dd3d-ea7f9a642c15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "total 17948\n",
            "drwxr-xr-x 1 root root    4096 Jul  9 15:43 .\n",
            "drwxr-xr-x 1 root root    4096 Jul  9 15:23 ..\n",
            "drwxr-xr-x 4 root root    4096 Jul  3 13:21 .config\n",
            "drwxr-xr-x 1 root root    4096 Jul  3 13:21 sample_data\n",
            "drwxr-xr-x 5 root root    4096 May  9  2023 samsum_dataset\n",
            "-rw-r--r-- 1 root root  530165 May  9  2023 samsum-test.csv\n",
            "-rw-r--r-- 1 root root 9401418 May  9  2023 samsum-train.csv\n",
            "-rw-r--r-- 1 root root  512131 May  9  2023 samsum-validation.csv\n",
            "-rw-r--r-- 1 root root 7903594 Jul  9 15:43 summarizer-data.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_samsum = load_from_disk('samsum_dataset')"
      ],
      "metadata": {
        "id": "bwS5ZRzrKi4U"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_samsum"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dZW6p0ysKu7b",
        "outputId": "9e0f0a97-e33c-41cb-c46b-cf2bba22aecb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 14732\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 819\n",
              "    })\n",
              "    validation: Dataset({\n",
              "        features: ['id', 'dialogue', 'summary'],\n",
              "        num_rows: 818\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "split_lengths = [len(data_samsum[split]) for split in data_samsum]\n",
        "\n",
        "print(f\"Split lengths: {split_lengths}\")\n",
        "print(f\"Features: {data_samsum['train'].column_names}\")\n",
        "print(\"\\nDialogue:\")\n",
        "\n",
        "print(data_samsum['test'][1]['dialogue'])\n",
        "\n",
        "print(\"\\nSummary:\")\n",
        "\n",
        "print(data_samsum['test'][1]['summary'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8FEhcMxjK0wk",
        "outputId": "4a23db09-d9b1-4a53-942d-a19763a1140d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Split lengths: [14732, 819, 818]\n",
            "Features: ['id', 'dialogue', 'summary']\n",
            "\n",
            "Dialogue:\n",
            "Eric: MACHINE!\r\n",
            "Rob: That's so gr8!\r\n",
            "Eric: I know! And shows how Americans see Russian ;)\r\n",
            "Rob: And it's really funny!\r\n",
            "Eric: I know! I especially like the train part!\r\n",
            "Rob: Hahaha! No one talks to the machine like that!\r\n",
            "Eric: Is this his only stand-up?\r\n",
            "Rob: Idk. I'll check.\r\n",
            "Eric: Sure.\r\n",
            "Rob: Turns out no! There are some of his stand-ups on youtube.\r\n",
            "Eric: Gr8! I'll watch them now!\r\n",
            "Rob: Me too!\r\n",
            "Eric: MACHINE!\r\n",
            "Rob: MACHINE!\r\n",
            "Eric: TTYL?\r\n",
            "Rob: Sure :)\n",
            "\n",
            "Summary:\n",
            "Eric and Rob are going to watch a stand-up on youtube.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def convert_examples_to_features(example_batch):\n",
        "    \"\"\"\n",
        "    Tokenize the raw text\n",
        "    \"\"\"\n",
        "    input_encodings = tokenizer(example_batch['dialogue'], max_length=1024, truncation=True)\n",
        "\n",
        "    target_encodings = tokenizer(text_target=example_batch['summary'], max_length=128, truncation=True)\n",
        "\n",
        "    return {\n",
        "        'input_ids': input_encodings['input_ids'],\n",
        "        'attention_mask': input_encodings['attention_mask'],\n",
        "        'labels': target_encodings['input_ids']\n",
        "    }"
      ],
      "metadata": {
        "id": "sOMtlpuINFWW"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_samsum_pt = data_samsum.map(convert_examples_to_features, batched=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 49,
          "referenced_widgets": [
            "c8c398c31884415489cd065b696416f6",
            "5f5bdf656a784f97aa922ad91156be9f",
            "a4fa5d88e91d4f5b9a12e4cb3eb0ae43",
            "3661f1db92864aa08a2b52d96721c7e7",
            "30894df989454bceac3f3f533a2233e5",
            "3771fd641bac4cf29c30b6f08f28eafa",
            "8d69125fb3e5486e8145b4a4c63002b4",
            "19c39f174f40438cb1e2807ad21e35ba",
            "14cfe78bcee445609d6be45b249a3ce2",
            "a72e3e6412694f13afdecc7f18ad6fc2",
            "08686dc6a601411496858fffd21cb65b"
          ]
        },
        "id": "u2Fg3W4hK-Am",
        "outputId": "0fcd4778-50f3-4ba5-cb6f-38cccf93547b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/819 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8c398c31884415489cd065b696416f6"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "data_samsum_pt['train']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ApsD0aGrO1tX",
        "outputId": "41a3b3d3-8809-4246-daf0-d66aae068740"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Dataset({\n",
              "    features: ['id', 'dialogue', 'summary', 'input_ids', 'attention_mask', 'labels'],\n",
              "    num_rows: 14732\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Training\n",
        "\n",
        "from transformers import DataCollatorForSeq2Seq\n",
        "\n",
        "seq2seq_data_collator = DataCollatorForSeq2Seq(tokenizer, model=model_pegasus)"
      ],
      "metadata": {
        "id": "FJBP5SzYPBjH"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "trainer_args = TrainingArguments(\n",
        "    output_dir='pegasus-samsum',\n",
        "    num_train_epochs=1,\n",
        "    warmup_steps=500,\n",
        "    per_device_train_batch_size=1,\n",
        "    per_device_eval_batch_size=1,\n",
        "    weight_decay=0.01,\n",
        "    logging_steps=10,\n",
        "    eval_strategy='steps',\n",
        "    eval_steps=500,\n",
        "    save_steps=1e6,\n",
        "    gradient_accumulation_steps=16\n",
        ")"
      ],
      "metadata": {
        "id": "jorfHjPJPlGW"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model_pegasus,\n",
        "    args=trainer_args,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=seq2seq_data_collator,\n",
        "    train_dataset=data_samsum_pt['test'],\n",
        "    eval_dataset=data_samsum_pt['validation']\n",
        ")"
      ],
      "metadata": {
        "id": "-McrwJzLQoEm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "torch.set_float32_matmul_precision('high')"
      ],
      "metadata": {
        "id": "dn1uF8i4RwQW"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "ZtlkxhFhRehw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "outputId": "4e931619-cc29-41e0-d5ae-3ecb55c8d783"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='51' max='51' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [51/51 02:59, Epoch 0/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=51, training_loss=3.0043694084765864, metrics={'train_runtime': 183.534, 'train_samples_per_second': 4.462, 'train_steps_per_second': 0.278, 'total_flos': 313450454089728.0, 'train_loss': 3.0043694084765864, 'epoch': 0.9963369963369964})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "\n",
        "def generate_batch_sized_chunks(list_of_elements, batch_size):\n",
        "    \"\"\"\n",
        "    split the dataset into smaller batches that we can process simultaneously.\n",
        "    Yield successive batch-sized chunks from list_of_elements.\n",
        "    \"\"\"\n",
        "    for i in range(0, len(list_of_elements), batch_size):\n",
        "        yield list_of_elements[i : i+batch_size]\n",
        "\n",
        "\n",
        "def calculate_metric_on_test_ds(dataset, metric, model, tokenizer,\n",
        "                                batch_size=16, device=device,\n",
        "                                column_text=\"article\",\n",
        "                                column_summary=\"highlights\"):\n",
        "    article_batches = list(generate_batch_sized_chunks(dataset[column_text], batch_size))\n",
        "    target_batches = list(generate_batch_sized_chunks(dataset[column_summary], batch_size))\n",
        "\n",
        "    for article_batch, target_batch in tqdm(\n",
        "        zip(article_batches, target_batches), total=len(article_batches)\n",
        "    ):\n",
        "\n",
        "        inputs = tokenizer(article_batch, max_length=1024, truncation=True,\n",
        "                        padding='max_length', return_tensors=\"pt\")\n",
        "\n",
        "        # parameter for length penalty ensures that the model does not generate seqeunces too long\n",
        "        summaries = model.generate(input_ids=inputs['input_ids'].to(device),\n",
        "                                attention_mask=inputs['attention_mask'].to(device),\n",
        "                                length_penalty=0.8, num_beams=8, max_length=128)\n",
        "\n",
        "        # Finally, we decode the generated texts,\n",
        "        # replace the token, and add the decoded texts with the references to the metric\n",
        "        decoded_summaries = [\n",
        "            tokenizer.decode(s, skip_special_tokens=True,\n",
        "                            clean_up_tokenization_spaces=True)\n",
        "            for s in summaries\n",
        "        ]\n",
        "\n",
        "        decoded_summaries = [d.replace(\"\", \" \") for d in decoded_summaries]\n",
        "\n",
        "        metric.add_batch(predictions=decoded_summaries, references=target_batch)\n",
        "        #print(\"\\nDecoded Summaries:\")\n",
        "        #print(decoded_summaries)\n",
        "        #print(\"\\nTarget Batches\")\n",
        "        #print(target_batches)\n",
        "    # Finally compute and return the ROUGE scores\n",
        "    score = metric.compute()\n",
        "    return score\n",
        "\n"
      ],
      "metadata": {
        "id": "qqw-VIeaTkCm"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_names = ['rouge1', 'rouge2', 'rougeL', 'rougeLsum']\n",
        "rouge_metric = load_metric('rouge')"
      ],
      "metadata": {
        "id": "fXYp8fxwY7MP"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score = calculate_metric_on_test_ds(\n",
        "    data_samsum['test'][0:10],\n",
        "    rouge_metric,\n",
        "    trainer.model,\n",
        "    tokenizer,\n",
        "    batch_size=2,\n",
        "    column_text='dialogue',\n",
        "    column_summary='summary'\n",
        ")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfw9Gwd3ZEH3",
        "outputId": "29fea62e-f6b3-49dc-9945-292d5ffaf23d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 5/5 [00:12<00:00,  2.58s/it]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "rouge_dict = dict((rn, score[rn].mid.fmeasure) for rn in rouge_names)\n",
        "\n",
        "pd.DataFrame(rouge_dict, index=[f'pegasus'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 81
        },
        "id": "nzqPJrq0ZqT9",
        "outputId": "3c71909c-6bb8-48f5-e4dc-2a6350247c3b"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "           rouge1  rouge2    rougeL  rougeLsum\n",
              "pegasus  0.020221     0.0  0.020213   0.020284"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-e8c6b8b4-143d-43ca-9a70-a092883d495f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>pegasus</th>\n",
              "      <td>0.020221</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.020213</td>\n",
              "      <td>0.020284</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-e8c6b8b4-143d-43ca-9a70-a092883d495f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-e8c6b8b4-143d-43ca-9a70-a092883d495f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-e8c6b8b4-143d-43ca-9a70-a092883d495f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"rouge1\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.020220849983060302,\n        \"max\": 0.020220849983060302,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.020220849983060302\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rouge2\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.0,\n        \"max\": 0.0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeL\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.020213310567614128,\n        \"max\": 0.020213310567614128,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.020213310567614128\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rougeLsum\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0.02028417956856001,\n        \"max\": 0.02028417956856001,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.02028417956856001\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save model\n",
        "model_pegasus.save_pretrained(\"pegasus-samsum-model\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g0kwQaWEcpyL",
        "outputId": "f2daefd7-4071-433d-f83d-da88b7d595b1"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some non-default generation parameters are set in the model config. These should go into a GenerationConfig file (https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model) instead. This warning will be raised to an exception in v4.41.\n",
            "Non-default generation parameters: {'max_length': 128, 'min_length': 32, 'num_beams': 8, 'length_penalty': 0.8, 'forced_eos_token_id': 1}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Save tokenizer\n",
        "tokenizer.save_pretrained('tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ry7B-Z23c1ui",
        "outputId": "01c0b091-70e9-46ca-9727-7189dc3a3c79"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('tokenizer/tokenizer_config.json',\n",
              " 'tokenizer/special_tokens_map.json',\n",
              " 'tokenizer/spiece.model',\n",
              " 'tokenizer/added_tokens.json',\n",
              " 'tokenizer/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/tokenizer\")"
      ],
      "metadata": {
        "id": "7QmyvJTmc7mC"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Prediction\n",
        "\n",
        "gen_kwargs = {\n",
        "    \"length_penalty\": 0.8,\n",
        "    \"num_beams\": 8,\n",
        "    \"max_length\": 128\n",
        "}\n",
        "\n",
        "pipe = pipeline('summarization',\n",
        "                model='pegasus-samsum-model',\n",
        "                tokenizer=tokenizer,\n",
        "                device=device)"
      ],
      "metadata": {
        "id": "_dPj_9cTdIuS"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_text = data_samsum['test'][2]['dialogue']\n",
        "\n",
        "reference = data_samsum['test'][2]['summary']\n",
        "\n",
        "print(\"Dialogue:\")\n",
        "print(sample_text)\n",
        "\n",
        "print(\"\\nReference Summary:\")\n",
        "print(reference)\n",
        "\n",
        "print(\"\\nModel Summary:\")\n",
        "print(pipe(sample_text, **gen_kwargs)[0][\"summary_text\"])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3jqogKMd-JN",
        "outputId": "615b4f55-d9d4-43b3-8524-f7f9320186d8"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dialogue:\n",
            "Lenny: Babe, can you help me with something?\r\n",
            "Bob: Sure, what's up?\r\n",
            "Lenny: Which one should I pick?\r\n",
            "Bob: Send me photos\r\n",
            "Lenny:  <file_photo>\r\n",
            "Lenny:  <file_photo>\r\n",
            "Lenny:  <file_photo>\r\n",
            "Bob: I like the first ones best\r\n",
            "Lenny: But I already have purple trousers. Does it make sense to have two pairs?\r\n",
            "Bob: I have four black pairs :D :D\r\n",
            "Lenny: yeah, but shouldn't I pick a different color?\r\n",
            "Bob: what matters is what you'll give you the most outfit options\r\n",
            "Lenny: So I guess I'll buy the first or the third pair then\r\n",
            "Bob: Pick the best quality then\r\n",
            "Lenny: ur right, thx\r\n",
            "Bob: no prob :)\n",
            "\n",
            "Reference Summary:\n",
            "Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
            "\n",
            "Model Summary:\n",
            "Lenny asks Bob which pair of trousers he should pick .<n>Bob says he already has purple trousers .<n>Lenny says he'll buy the first or the third pair of trousers .\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "J9Sdyorud4BV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}