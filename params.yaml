TrainingArguments:
  dataset_text_field: document
  num_train_epochs: 1
  per_device_train_batch_size: 1
  per_device_eval_batch_size: 1
  gradient_accumulation_steps: 16
  learning_rate: 2e-4
  lr_scheduler_type: cosine
  logging_steps: 10
  max_seq_length: 1024
  lora_r: 32,
  lora_alpha: 16,
  lora_dropout: 0.1,
  lora_target_modules: ['k_proj', 'v_proj', 'q_proj', 'out_proj']